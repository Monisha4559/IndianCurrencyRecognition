# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17S5cRzWyTRWVykf88pO4gAcTy0mF_cbB
"""

# main.py
import os
import zipfile
import numpy as np
import matplotlib.pyplot as plt
from skimage.io import imread
from skimage.transform import resize
from skimage.feature import hog
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils import resample
from sklearn.utils.class_weight import compute_class_weight
import seaborn as sns
import joblib
from google.colab import files

# ------------------- 1Ô∏è‚É£ Upload zip -------------------
print("üîπ Upload your demo_currency_dataset.zip file")
uploaded = files.upload()  # Upload the zip

DATA_ZIP = list(uploaded.keys())[0]
WORK_DIR = "/content/demo_currency_dataset"

# ------------------- 2Ô∏è‚É£ Unzip -------------------
with zipfile.ZipFile(DATA_ZIP, 'r') as zip_ref:
    zip_ref.extractall(WORK_DIR)
print(f"‚úÖ Dataset unzipped to {WORK_DIR}")

# Automatically detect the inner folder
inner_dirs = [f for f in os.listdir(WORK_DIR) if os.path.isdir(os.path.join(WORK_DIR, f))]
if len(inner_dirs) == 1:
    DATA_DIR = os.path.join(WORK_DIR, inner_dirs[0])
else:
    DATA_DIR = WORK_DIR
print("Using dataset folder:", DATA_DIR)
print("Classes found:", os.listdir(DATA_DIR))

# ------------------- 3Ô∏è‚É£ Paths for outputs -------------------
MODEL_PATH = "currency_pipeline.pkl"
CONF_MATRIX_PATH = "confusion_matrix.png"
REPORT_PATH = "classification_report.txt"

# ------------------- 4Ô∏è‚É£ Load images -------------------
X, y = [], []
target_size = (64, 64)

for label in os.listdir(DATA_DIR):
    folder = os.path.join(DATA_DIR, label)
    if not os.path.isdir(folder):
        continue
    for file in os.listdir(folder):
        img_path = os.path.join(folder, file)
        if not os.path.isfile(img_path):
            continue
        try:
            img = imread(img_path, as_gray=True)
            img_resized = resize(img, target_size, anti_aliasing=True)
            features = hog(
                img_resized,
                orientations=9,
                pixels_per_cell=(8, 8),
                cells_per_block=(2, 2),
                block_norm="L2-Hys"
            )
            X.append(features)
            y.append(label)
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping {img_path}, not a valid image")

X = np.array(X)
y = np.array(y)
if len(X) == 0:
    raise ValueError("No images loaded! Check your zip file and folder structure.")
print(f"‚úÖ Feature matrix: {X.shape}, Labels: {len(y)}")

# ------------------- 5Ô∏è‚É£ Train/Test Split -------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print("Train:", X_train.shape, "Test:", X_test.shape)

# ------------------- 6Ô∏è‚É£ Balance classes -------------------
classes = np.unique(y_train)
max_count = max([np.sum(y_train == cls) for cls in classes])

X_train_bal, y_train_bal = [], []
for cls in classes:
    X_cls = X_train[y_train == cls]
    y_cls = y_train[y_train == cls]
    X_res, y_res = resample(X_cls, y_cls, replace=True, n_samples=max_count, random_state=42)
    X_train_bal.append(X_res)
    y_train_bal.append(y_res)

X_train = np.vstack(X_train_bal)
y_train = np.hstack(y_train_bal)
print("Balanced Train:", X_train.shape)

# ------------------- 7Ô∏è‚É£ Class weights -------------------
class_weights = dict(zip(
    classes,
    compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
))
print("Class weights:", class_weights)

# ------------------- 8Ô∏è‚É£ Define models -------------------
models = {
    "SVC_rbf": (SVC(probability=True, class_weight=class_weights), {
        "clf__C": [1, 10, 50],
        "clf__gamma": ["scale", 0.01, 0.001]
    }),
    "RandomForest": (RandomForestClassifier(random_state=42, class_weight=class_weights), {
        "clf__n_estimators": [200, 300],
        "clf__max_depth": [None, 20, 30]
    }),
    "KNN": (KNeighborsClassifier(), {
        "clf__n_neighbors": [3, 5, 7]
    })
}

# ------------------- 9Ô∏è‚É£ Train & Evaluate -------------------
results = []
best_model = None
best_acc = 0

for name, (clf, grid) in models.items():
    print(f"\nüîç Training {name} ...")
    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=0.95, random_state=42)),
        ("clf", clf)
    ])
    cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)
    gs = GridSearchCV(pipe, grid, cv=cv, scoring="accuracy", n_jobs=-1)
    gs.fit(X_train, y_train)

    best_pipe = gs.best_estimator_
    cv_mean = gs.best_score_
    test_acc = accuracy_score(y_test, best_pipe.predict(X_test))

    results.append({"name": name, "cv_mean": cv_mean, "test_acc": test_acc})

    if test_acc > best_acc:
        best_acc = test_acc
        best_model = best_pipe

# ------------------- 10Ô∏è‚É£ Report -------------------
print("\nüìä Model Performance:")
print(f"{'Model':<15} {'CV Mean':<10} {'Test Acc':<10}")
for res in results:
    print(f"{res['name']:<15} {res['cv_mean']*100:>6.2f}%   {res['test_acc']*100:>6.2f}%")

# Save model
joblib.dump(best_model, MODEL_PATH)
print("‚úÖ Best model saved at", MODEL_PATH)

# Classification report
y_pred = best_model.predict(X_test)
report = classification_report(y_test, y_pred, output_dict=True)
with open(REPORT_PATH, "w") as f:
    for label, metrics in report.items():
        if isinstance(metrics, dict):
            f.write(f"\nClass {label}:\n")
            for m, v in metrics.items():
                f.write(f"  {m}: {v*100:.2f}%\n")
print("‚úÖ Classification report saved at", REPORT_PATH)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.savefig(CONF_MATRIX_PATH)
plt.show()
print("‚úÖ Confusion matrix saved at", CONF_MATRIX_PATH)